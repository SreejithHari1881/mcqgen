{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (0.2.17)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (3.10.10)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: langchain<0.3.0,>=0.2.16 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (0.2.16)\n",
      "Requirement already satisfied: langchain-core<0.3.0,>=0.2.39 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (0.2.41)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (0.1.136)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (1.24.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain_community) (8.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.15.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (0.2.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain<0.3.0,>=0.2.16->langchain_community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain_community) (24.1)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langchain-core<0.3.0,>=0.2.39->langchain_community) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (0.27.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from langsmith<0.2.0,>=0.1.112->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from requests<3,>=2->langchain_community) (2024.8.30)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (4.5.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.0.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.39->langchain_community) (3.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.16->langchain_community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.112->langchain_community) (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key= KEY, model_name='gpt-4', temperature=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (3.0.1)\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in c:\\users\\91897\\mcqgen\\env\\lib\\site-packages (from PyPDF2) (4.12.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=['text', 'number', 'subject', 'tone', 'response_json'],\n",
    "    template= TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_chain = LLMChain(llm= llm, prompt= quiz_generation_prompt, output_key= 'quiz', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "                                        output_variables=[\"quiz\", \"review\"], verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"C:\\Users\\91897\\mcqgen\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\91897\\\\mcqgen\\\\data.txt'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    Text= file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video 1: Supervised Learning Techniques: Overview\n",
      "Hello everyone. Welcome to the module on supervised learning techniques. Machine Learning typically use historical data to build models. Supervised learning is used for solving classification or prediction problems with the help of labelled data. The data collected from a source or from different sources may not be directly usable for building machine learning models.\n",
      "So, this data needs to undergo pre-processing which involves multiple steps including data cleaning, data integration, data reduction and so on. The performance of ML classifier is evaluated using a number of metrics such as accuracy, precision, recall and so on. This module demonstrates the classification techniques in machine learning with the help of two baseline classifiers such as SVM classifier and Naive Bayes classifier.\n",
      "Video 2: The Context of Machine Learning\n",
      "AI/ML is one of the most important strategic technologies now. For most of the industries, AI/ML is part of their business strategy, IT strategy or both. Businesses and IT professionals are finding amazing use cases for AI/ML and already experiencing its benefits, but still a lot more to do to realise the potential of AI/ML. Positive impacts of AI/ML are reported from businesses on brand awareness, revenue generation and expense reduction. More and more companies are increasing their overall AI/ML budgets.\n",
      "IT departments generally have a strong understanding of AI/ML, but the leadership or other departments do not understand it well. Thus, there is a need to make the stakeholders understand AI/ML better. The current uses of AI/ML in management include optimisation of the processes to improve their speed and efficiency, understanding the employees better, personalising the content to the customers and understanding the customers better, increasing the revenues, gaining competitive advantage, predicting business performance and industry trends, understanding market effectiveness and reducing the risk.\n",
      "Video 3: Supervised Learning\n",
      "Let us start with Supervised Learning. The supervised learning approach is similar to the human learning under the supervision of a teacher. The teacher provides good examples to the student to understand and then the student derives general rules from the examples. In\n",
      "2\n",
      "supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs.\n",
      "Consider an example of fruits, where we train the ML model using the data, which is labelled for classification as apples and oranges. Once the training is complete, the machine is ready to classify new fruits data without label. Look at some examples of supervised learning classification, we have a label dataset of images of cats and dogs. Now, if we have a model to identify if the next image is a dog or a cat.\n",
      "This is an example for binary classification. We have a set of movie reviews with sentiment label as good, neutral or bad. Now, identify a new review sentiment as good, neutral or bad. This is an example for multiclass classification. We have images of handwritten digits from 0 to 9. Now, identify a number from hand-drawn digits image. This is an example for multiclass classification.\n",
      "The supervised learning process. We have a basket filled with two kinds of fruits, say apple and banana. The first step is to train the model with all different fruits one by one using two features: The first feature is shape; rounded with the depression at the top. The second feature is Color; red. If it is rounded with the depression at the top and the color is red, then it is apple. If the shape is long curving cylinder and the color is green or yellowgreen color, then it is banana.\n",
      "Now, the model is ready to classify a new fruit. It will classify the fruit using its shape and color as banana or apple. Thus the machine learns from training data, which is the basket containing fruits, and then applies this knowledge to test new data which could be in the new basket.\n",
      "Another supervised task is to predict a target numeric value such as the price of a used car; given the set of features such as mileage, age, brand and so on. These features are called Predictors. This sort of task is called Regression. Here to train the model, we need to give it many examples of cars, including their predictors and the labels. Here, the labels are the prices of these cars.\n",
      "Now, let us look at the types of supervised learning. Supervised learning techniques can be classified into two: classification and prediction. The purpose of classification is to predict the class of objects whose class label is unknown. Here we are categorising the new incoming data based on our assumptions and the data we have with us. In prediction, we predict the data based on the previous data we have with us and based on the assumptions. For example, we can forecast the likelihood of a particular outcome, such as whether or not a customer will churn in 30 days.\n",
      "Video 4: Classification Tasks\n",
      "So, in this video, we'll be talking about the classification class. There are four types of classification tasks that we may encounter in real life. These are binary classification, multiclass classification, multi-label classification and imbalanced classification. Binary classification is the task when we have two class labels.\n",
      "3\n",
      "Example, cancer detection, cancer patient or not. Conversion prediction, buy or not. Email spam detection, spam or harm. Binary classification tasks typically involve a normal state and an abnormal state which are the two classes. For example, not spam is the normal state and spam is the abnormal state. Similarly, cancer not detected is the normal state and cancer detected is the abnormal state. The class for the normal state can be assigned a class label one and the class with abnormal state can be assigned a class label zero.\n",
      "The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome either as a zero or one. We can model a binary classification task with a model that predicts a Bernoulli probability distribution for each example. This means that the model predicts the probability of an example belonging to the abnormal state. Popular binary classification algorithms include naive bayes, support-vector machine, logistic regression, K-nearest neighbours and decision trees.\n",
      "The logistic regression and support-vector machines do not support more than two classes. Now, let us look at the multiclass classification. Multiclass classification involves classification tasks that have more than two class labels. Examples include optical character recognition, face recognition, plant species classification. In multiclass classification, there is no notion of normal and abnormal outcomes.\n",
      "Here is an example, which is classified as belonging to one among a range of non classes. In face recognition, the model may be predicting a photo as belonging to one among thousands of faces in the face recognition system. Predicting a sequence of words in text translation models is a type of multiclass classification. Here, the size of the vocabulary defines the number of classes that may be predicted and could be many thousands of words in size.\n",
      "A multiclass classification task can be modelled with a model that predicts a multinolli probability distribution for each example. This means that the multiclass classification model predicts the probability of an example belonging to each class label. The popular multiclass classification algorithms include naive bayes, K-nearest neighbours, decision trees, random forest and gradient boosting. The third type is the multi-label classification.\n",
      "In multi-label classification, there will be two or more class labels where one or more class labels may be predicted for each case. For example, in image classification, a given image may have multiple objects in a scene and the multi-label model may predict the presence of multiple known objects in the image such as bike, car, person and so on. In binary classification and multiclass classification, only a single class label is predicted in each case.\n",
      "We can model a multi-label classification task with a model that predicts multiple outputs with each output prediction coming from a Bernoulli probability distribution. Multi-label versions of the standard classification algorithms are needed for multi-label classification, such as multi-label decision trees, multi-label gradient boosting, multi-label random forest. In imbalanced classification, the number of examples in each class is imbalanced or unequally distributed.\n",
      "Typically, they are binary classification tasks. The majority of the examples in the training data set will belong to the normal class and only a minority of the examples will be in the abnormal class. For example, brain tumor detection or financial fraud detection or outlier\n",
      "4\n",
      "detection. In each of this case, we can see that the abnormal state will be less in number. The imbalanced classifiers are modelled as binary classification task with specialised techniques.\n",
      "These specialised techniques will change the composition of the samples in the training data set by undersampling the majority class, or oversampling the minority class. Undersampling would decrease the proportion of the majority class until the number matches to the minority class. Oversampling would resemble the minority class proportion to match the majority class proportion.\n",
      "Video 5: Data Collection\n",
      "In this video, we'll discuss the data collection techniques. Data collection is the process of gathering data from different sources. To be useful for machine learning, this data must be collected and stored in a way that makes sense for the business problem at hand. So, the data has to be relevant.\n",
      "Humans and machines are generating lots of data. This data include numeric data, categorical data or text data. We can use primary or secondary data for building machine learning models. The primary data sources include surveys, observations, questionnaires, experiments, personal interviews and so on. The secondary data sources can be public data sets, research reports, government publications, websites, publications from independent research labs and so on. How to collect data if we don't have any primary data with us?\n",
      "The possibilities are rely on open source data sets, Google data set search, Microsoft research open data, Amazon data sets, Kaggle data sets, UCI data sets where UCI stands for University of California at Irvine. Public data sets include mall customer data set, which contains information about people visiting the mall in a particular city. Another public data set is MNIST, Modified National Institute of Standards and Technology data set, which contains handwritten digits from 0-9.\n",
      "Another public data set is the Boston Housing data set, which contains data collected by the US census service concerning housing in the Boston area. Other public data sets include: Fake News Detection data set, Wine quality data set, SOCR which stands for Statistics online computational resource data which is about the heights and weights of people, then credit card fraud detection data set and COVID-19 data set.\n",
      "Video 6: Data Preprocessing\n",
      "This video explains you data preprocessing. There could be a lot of data for solving a problem, but the data available could be dirty. So, we cannot apply this data to our machine learning algorithms directly. If we use dirty data to develop the machine learning model, then the outcome will also be dirty. There is a saying, \"Garbage in, garbage out\", which will be true with respect to data in machine learning models.\n",
      "5\n",
      "So, there is a need to have a data preprocessing in every machine learning solution, so that the correct data is supplied to the model. This is a necessary requirement but not sufficient to have a great model. The machine learning model outcome can be bad with good data also if we choose the wrong machine learning model. So, we need to have both good data as well as the right machine learning model for a particular problem. Data preprocessing is divided into four stages: Data cleaning, Data integration, Data reduction, and Data transformation.\n",
      "Data cleaning is the process of identifying or removing incorrect, corrupted, incorrectly formatted, duplicated or incomplete data from a data set. Data integration is the process of bringing together data from different sources, both traditional and non-traditional into a meaningful data set for analytics.\n",
      "Data reduction techniques are used to preserve data in a reduced or condensed form without loss of information. Data transformation is the process of converting the raw data into a format that would be suitable for building a machine learning model. Let us begin with data cleaning step. The number of steps in data cleaning depends on the cleanliness of the incoming data.\n",
      "Data cleansing involves detecting and correcting corrupt or inaccurate records from a set of records or table. So, there has to be mechanisms to identify incomplete, incorrect, inaccurate or irrelevant parts of the data and then replace, modify or delete the dirty data.\n",
      "Data imputation is a technique use for replacing the missing data with some substitute value to retain most of the data in the data set. This is needed because removing the data from the data set every time is not feasible and can lead to a reduction in the data size, which may cause concerns for biases.\n",
      "The approaches to data imputation are the following: The first thing is do nothing. Some of the algorithms automatically takes care of this missing data. The second possibility is imputation using the median or mean values. A third technique is imputation using the most frequent values. Another technique is using zero or constant values. Then, the possibilities are using algorithms like K-nearest neighbours or the use of multi-variant imputation by chained equation which is known as MICE and also imputation using deep learning.\n",
      "Now let us look at data integration techniques. Here data integration solution integrates the data collected from multiple sources which could be on-premise or from cloud sources. The data integration should address the security, privacy and data quality issues also. It uses techniques such as ETL and ELT. ETL stands for extract, transform and loading data from multiple data sources to a single data store, which is then loaded into the data warehouse.\n",
      "ELT stands for extract and load raw data from multiple data sources into a target data lake or cloud data warehouse where the data can be transformed when needed. ELT is more appropriate for machine learning applications. Data reduction is the transformation of data obtained empirically or experimentally into an optimal or simplified form for analysis. Three types of data reduction exist which include feature reduction, case reduction and value reduction.\n",
      "Feature reduction reduces the number of columns, case reduction reduces the number of rules whereas value reduction reduces the numeric value. More input features or columns\n",
      "6\n",
      "make a predictive modelling task more challenging to model and is generally referred to as the curse of dimensionality.\n",
      "Principal component analysis, PCA, Factor analysis, FA, Linear discriminate analysis, LDA and Truncated singular value decomposition, SVD, are examples of feature reduction techniques. Next is data transformation. Data transformation is the process of converting the raw data into a format or structure that would be suitable for machine learning model building.\n",
      "This may include data smoothing, data aggregation, discretisation, generalisation, attribute construction and normalisation. Data smoothing is used for removing the noise from a data set. Noise is the distorted and meaningless data within a data set.\n",
      "Data aggregation is the process of collecting data from multiple sources and storing it using a single format. Data discretisation is a process of converting continuous data into a set of data intervals. This may make the data analysis easier. Data generalisation is the process where low-level data attributes are transformed into high-level data attributes using concept hierarchies.\n",
      "This is useful to get better insights about the data. For example, age data in a data set can be transformed into categorical values such as young, middle-aged, old and so on. Attribution construction allows new attribute creation from the existing set of attributes.\n",
      "For example, the attributes such as employee name, employee ID and address from one data set can be used to construct another data set that contains information about the employees who joined in a particular year, for instance, 2021. Data normalisation transforms the data, so that all data falls within a given range. The popular data normalisation methods include: Min-max normalisation, decimal scaling and Z-score normalisation.\n",
      "Video 7: Data Encoding\n",
      "This video explains you techniques for data encoding. Machine learning algorithms can understand only numbers. So, for handling text, the text or string columns must be converted into numerical columns. For example, if the gender column has values like male or female, we need to map them to numerical values.\n",
      "This conversion process is called encoding. We can use One-hot encoding and Label encoding which is available in scikit-learn for encoding categorical columns into numerical columns. Let us look at label encoding first. Label encoding is simple when we assign a numerical value to each value in the categorical column value. Each value in the categorical column is called label. For example, gender column can be labelled as zero for female and one for male. This means that we can simply replace male by one and female by zero.\n",
      "But if you want to assign labels to the continents of the world, it needs seven values. That means from zero to six. This may end up giving more weightage to particular values when we compare these numerical values. This is a drawback of label encoding. Now let us look at one-hot encoding.\n",
      "This is called one-hot encoding because only one index has a non-zero value. The value one is called 'hot' and the value zero is called 'cold'. When we apply one hot encoding to\n",
      "7\n",
      "represent the seven continents in the world, we will have a seven-bit vector. In this vector only one of the seven bits can be a numeric value one at a time which corresponds to a continent. For example, Africa being first in the lexicographic ordering can be coded as one followed by six zeroes, whereas South America being the last in the lexicographic ordering can be coded as six zeroes followed by one.\n",
      "Video 8: Performance Metrics of Classifiers\n",
      "This video explains you the performance metrics of classifiers classification is the process of predicting the class of given data points. There are two types of learners in classification, lazy learners and eager learners. Lazy learners simply store the training data and wait until a testing data appears. When the data appears.\n",
      "Classification is done based on the most related data in the store training data. Lazy learners need less training time but takes more time in predicting. An example is the K-nearest neighbour classifier. Eager learners construct a classification model based on a given training data before receiving the data for classification, eager learners take a long time for training and less time to predict.\n",
      "Examples include decision tree bayes and artificial neural network classifiers. Many classification algorithms are available now but it is not possible to conclude which one is superior to the other. The suitability of an algorithm depends on the application and nature of the available data set. For evaluating a classifier we have two prominent methods. These are the holdout method and the cross- validation method. In the holdout method, the given data set is divided into two partitions as test and training data sets.\n",
      "This could be 20:80 ratio or 30:70 ratio, or it could be 90:10 ratio also depending on the size of the data available, we can have other combinations. Also the train set will be used to train the model and the unseen test data will be used to test the models predictive power. In cross- validation, the data cities randomly partitioned indu que mutually exclusive subsets each approximately of equal size and one is kept for testing while others are used for training this process is iterated throughout the whole K-fold. K-fold Cross validation in general eliminates the overfitting issues. Overfitting occurs when a statistical model fits exactly against its training data. When this happens, the algorithm cannot perform accurately against unseen data.\n",
      "The sign of overfitting is when air around testing set is much greater than the air around training set. Under fitting happens when the model cannot capture the underlying trend of the data. It occurs when the model is too simple. The sign of under fitting is error on both training and test data. The performance of a classification model can p assist using accuracy, which is the percentage of correct predictions. However accuracy depends on the data. For example, considering binary classification model using a logistic regression with an accuracy of 95%.\n",
      "8\n",
      "This model should be very good But the data was imbalanced where 95% of the records for Class zero and 5% records of Class one and the model was a poor model And was crediting zero for all the cases but we get accuracy is 95%. This is not correct. So we need to identify other evaluation metrics also depending upon the data and problem at hand. Another metric for classifications is the arrow sickle which overcomes some of the problems faced with accuracy. R. O. C. Stands for receiver operator characteristics which is a probability curve that has a false positive rate fpr at the X axis and the true positive rate T P R at the Y axis.\n",
      "It is primarily used for binary classification model evaluation. When Area under the Curve, a U. C. is one. Then the classifier is able to perfectly distinguish between all the positive and negative class points correctly. When a U.C. is between .5 and one there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. When you see is exactly .5. Then the classifier is not able to distinguish between positive and negative class points. If you see zero then the classifier would be predicting all negatives as positives and all positives as negatives. Four types of outcomes are possible while evaluating the performance of a classification model.\n",
      "True positives, tp true negatives, tiene false positives FP And false negatives or false positives is also known as Type one error. False negatives is also known as Type two error. A confusion matrix is a tool for summarizing the performance of a classification algorithm. It gives us a clear picture of the performance of a classification model and the types of errors it produces. It is giving the correct and incorrect predictions broken down by each category. The summary is represented in a tabular form for evaluating machine learning classifiers.\n",
      "The following five criteria are used more commonly classification accuracy, precision recall, a phone score and area under the R. O. Seeker. The precision score represents the model stability to currently predict the positives out of all the positive predictions it makes procession is calculated us true positive divided by some of false positives and true positives. Recall measures how good our Ml model is at identifying all actual positives out of all positives that exist within their data set. Recall is calculated by dividing true positive rates by the sum of false negatives and true positives.\n",
      "The accuracy tells us how often we can expect our Ml model will correctly predict an outcome out of the total number of times it makes predictions, accuracy is calculated by dividing the sum of true positives and true negatives by the sum of true positives, false negatives. True negatives and false positives. Their phone score is used as a single value that provides high level information about the model's output quality. A phone score is calculated bye Dividing the product of precision and recall by the sum of precision and recall multiple. eight x 2\n",
      "9\n",
      "Video 9: SVM Classifier\n",
      "This video explains you the SVM classifiers. A Support Vector Machine or SVM, is a supervised machine learning algorithm which can be used for both classification and regression problems, but majorly used for classification.\n",
      "Support vector machine constructs a line or a hyperplane in a high or infinite dimensional space, which is used for classification, regression, or other tasks like outlier detection. Support vector machine makes sure that the data is separated with the widest margin. Support vectors are data points that are close to the hyperplane that determines the orientation and position of the hyperplane. Support vector machines also draws two parallel hyperplanes called marginal planes to this hyperplane, such that they touch the nearest positive and negative points. The distance between the main hyperplane and the two parallel hyperplanes is called marginal distance.\n",
      "The marginal planes help in generalising the model, preventing overfitting. The goal of support vector machine is to train a model that assigns new unseen objects into a particular category. Support vector machines does this classification by creating a linear partition of the feature space into two categories based on the features in the new unseen objects, it places an object above or below the separation plane leading to a categorisation. Support vector machines is a non-probabilistic linear classifier because the features in the new objects fully determine its location in feature space and there is no stochastic element involved.\n",
      "Video 10: SVM Classifier: Demonstration\n",
      "This video is a demonstration of SVM classifier. So, what we do is we are trying to predict diabetes using a data set that has some predictor variables. So, what we are doing is we are trying to predict diabetes of patients using features like BP, BMI, insulin level, skin thickness and so on. So, we begin with importing the necessary libraries.\n",
      "You can see that Pandas, NumPy, Matplotlib, Seaborn. So all these libraries are imported, scikitk-learn also. This is the Pima Indians Diabetes Database. Using the read_csv command, we are reading the file data sets_228_482_diabetes. Now, we look at the sample data. So, you can see that these are the columns. Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and Outcome. So, Outcome is the prediction or the label as diabetic patient or not.\n",
      "Now, we use the describe() command to get the descriptive statistics of the data, and you will get count, mean, std, min then the quartiles and max. And the size of the data 768 rows and 9 columns. We are checking the missing values using this statement. Our data frame is df.isnull() and we are getting false. That means there are no null values. Now, we don't want zero values.\n",
      "So, replace all zeros by not a number. Then splitting data into tests and train data sets and the test_size is 0.2, that means 20% test size and 80% training size. Now, we use the support vector machine. We fit the model, and now we check the accuracy of the model. And we get\n",
      "10\n",
      "an accuracy of 0.79. Now, let us look at the confusion_matrix. Because accuracy alone won't be a good measure, so we look at other performance metrics as well.\n",
      "So, we have the Confusion matrix and the True Positives, True Negatives,\n",
      "False Positives and False Negatives are here. And we see that False Positives is good but False Negatives is not good because you have 23 that and True Negatives is only 24. Now, we use the Seaborn to display the confusion matrix.\n",
      "Now, look at the confusion matrix. We can see that the true positive rate is 98, and true negatives is 24. The predicted negative where it is actually positive. That means this is false negative which is 23 and the false positive is nine. So, since the false negative is 23 and true negative is only 24, the confusion matrix shows that this model is not very good. Now, you look at the classification_report, the precision, recall, f1-score and accuracy score. So, you can see that if you look at for 0, that means normal condition, precision, recall and f1-score are good.\n",
      "But whereas, the abnormal condition, that is diabetic condition, precision is only 0.73, recall is 0.51 and f1-score is 0.60. Accuracy is 0.79. So, recall and f1_score are not very good for diabetic condition. Now, we are computing each of this metric individually. So, Classification accuracy, the exact value is 0.7922. The Classification error is calculated which is 0.2078, the Precision is 0.9159, Recall 0.8099, the True Positive Rate is 0.8099, False Positive 0.2727, Specificity 0.7273 and the ROC AUC score is computed as 0.7133. Anything more than 0.7 is good, so this is a reasonably good value.\n",
      "Video 11: Naïve Bayes Classifier\n",
      "This video explains you the Naive Bayes classifier. Naive Bayes classifier is based on the Bayes' theorem. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. It is called Naive because the above assumption may or may not be true. Naive Bayes classifier is simple and very effective in building fast machine learning models for classification. Bayes' theorem gives the probability of an event occurring, given the probability of another event that has already occurred.\n",
      "So, it can be mathematically stated as: P(A|B)= (P(B|A)*P(A))/P(B), where A and B are events and P(B) is not equal to zero. P(A) is the prior probability, that is, the probability of the event before evidence is seen. And, P(A|B) is a posterior probability of B, that is, probability of event after the evidence is seen. So, we have an example. Event A: Diagnosed as a liver patient and Event B: Diagnosed as alcoholic.\n",
      "Now, we have patient data where 10% of patients are coming for treatment are having liver disease, that is, P(A) is 0.1. And, 5% of the patients coming for treatment are alcoholic, that means P(B) is 0.05.\n",
      "11\n",
      "Also those who have liver disease, it is known that 7% are alcoholic, that means P(B|A) is 7%. Now by applying Bayes' theorem, we can calculate the P(A|B), which is (0.07* 0.1)/0.05, which is equal to 0.14. That is, among the alcoholics, the chances of having liver disease is 0.14, that means 14%. This is more than the 10% suggested by the existing patient data. Types of Naive Bayes classifiers.\n",
      "The first one is multinomial Naive Bayes. The features and predictors used for document classification are the frequency of the words present in the document. The second type is Bernoulli Naive Bayes. Here, the predictors are Boolean variables, such as taking values 'yes' or 'no'. So, if used for document classification, it says a word occurs in the text or not. The next is Gaussian Naive Bayes. Here, the predictors take up continuous values.\n",
      "Video 12: Naïve Bayes Classifier: Demonstration\n",
      "This video is a demonstration of the Naive Bayes Classifier. We are building a Naive Bayes Classifier to predict whether a person makes over 50k per annum or not. So, our file is async2_naive-bayes-classifier. We begin with importing the files. Now, we are importing the data set, that is, Adult csv. We are importing the file adult.csv.\n",
      "So, this is an adult income data set. It's a CSV file, read it into a DataFrame df. Now, we make a exploratory data analysis first looking at the dimensions of the data set using the .shape. So, this gives that there are 32,561 rows for this data and 15 column. Now, let us look at the top five rows of this data set. And you can see that the columns are not labelled.\n",
      "So, we need to rename the columns. So, these are the columns: age, workclass, functional weight, education and many more. So, all these are assigned as column names. Now, we print the columns for this DataFrame and once again look at the DataFrame after assigning the columns. Now, the data set has a decent look. So, this is also our data cleaning process. Now, we get a summary of the data set. We have 15 features here and showing that all these 32,561 are non-null.\n",
      "There are different data types, integer, object data types. Now, let us look at the categorical variables. There are nine categorical variables, and we'll look at the first 5 rows with only the categorical variables and these are the ones. So, the last column is income where it is less than or equal to 50k. So, what it gives is there are nine categorical variables, and these are the names, and income is the target variable. Now, we check for missing values.\n",
      "So, it says that there are no missing values. Then we check the frequency counts. So, each of this categorical variable, you can see the frequency count and you can see a question mark also here. So, the percentage-wise distribution we can see. So here, what we see that several variables have missing values. Generally, these missing values are coming with NaN code, but here it is not NaN.\n",
      "The missing values are coded as question mark. That's why our check was giving that there are no missing values. So, you can see that at multiple places you have the question marks. So, all those are missing values. Now, let us explore the workclass variable.\n",
      "12\n",
      "So, the unique variables are this and you see the question mark also here. Now, let us look at the frequency distribution of the values in this workclass variable and the question mark, there are 1,836 positions. These are to be replaced with NaN. Now again we are checking the frequency distribution. Now, we see that the question mark is not there. Now, we explore the occupation and native_country columns also.\n",
      "So, these are the unique variables in that occupation. Then check out the value counts. Here also, you see the question mark, 1,843. Replace it with NaN. Onc again, you check, now it's not there. Similarly, for the native_country variable. native_country, you have multiple countries, from United States to Holland-Netherlands. Frequency distribution. So here, the question mark there are 583. Replacing with NaN. Now, the question mark is not there.\n",
      "So, once again, checking for missing values in the categorical variables. Now, we see that all their NaNs are coming as missing values. So, workclass, we have replaced 1,836 question marks to NaN, occupation, 1,843 and native_country, 583. All these are now appearing us null.\n",
      "Now, we need to explore the numerical variables also. So, these are the numerical variables, and we explore them. Now, check for missing values. So, no missing values in that. Now, we are separating out the target variable that is income. So, this is assigned to X, And income alone is Y. Now, we declare the feature vector and target variable. The X variables are by dropping the income, and Y is the income variable. Now, we split the data into separate training and test data set. So, we use the Scikit-learn train_test_split.\n",
      "Now, you see that the test size is 0.3, that means 30% test data and 70% training data. Now, we are checking the shape of both training and test data. So, we can see that the training data has 22,792 rows and, of course, 14 columns, whereas test data has 9,769 rows. Now we check the data types in x_train. These are the data types. And we display the categorical variables. These are the categorical variables, and these are the numerical variables. Now, we check for the missing values in the categorical variables in the training data.\n",
      "So, these are the percentage of missing values. You can see that in workclass, occupation, and native_country there are missing values. Now, we are printing the categorical variables with missing values. And we are going to impute the missing categorical variables. We are going to drop them but rather we impute them with the most frequent values from the data set. So, for each of the variables workclass, occupation, native_country, we fill with the most frequent value from the data set. Now, we are checking again the missing values. We see that nothing is left out. So, no missing values now.\n",
      "So, test data, we are checking for the missing values. So, there are no missing values. So, checking missing values in x_train, no missing values. x_test, no missing values. Once we verify that there are no missing values in the training and test data, now, we can encode the categorical variables.\n",
      "13\n",
      "So, these are the categorical variables. And we display the first five rows of that. We need to install the category_encoders using the pip install command, pip install category_encoders. If we don't do it, we'll end up in an error. So, we'll install it then import the category_encoders as ce. Then we go for the one-hot encoding of the category variables. We have five rows and 105 columns.\n",
      "Now, check the shape of the training data. Now, you see that there are 22,792 rows 105 columns after one hot encoding. Now, we are checking the first five rows of the test data. Five rows and 105 columns. Now, we are checking the first five rows of the test data. There are 9,769 rows 105 columns. So, both training and test data are ready for modern building. Now, we need to bring all the feature variables to the same scale. This is called feature scaling.\n",
      "So, we import the RobustScalar. Now, after applying the scaler, we see that the values are brought down to the same scale. If you look at the previous values of these variables, you can see that age, numerical values and a fnlweight. Also, you can see the numerical values. Once you scale it, all those are brought to the same scale.\n",
      "So, after scaling all these values are brought to the same scale. That is between zero and one for absolute values. Now, we'll train the Gaussian Naive Bayes classifier using the training data set. So, training is complete. Now we use the test data set for prediction. So, after testing, we check the accuracy score.\n",
      "Accuracy score is 0.8083. That is pretty good. And we need to check the training set accuracy score that is also around 80%, 0.8067. So, since both training accuracy and test accuracy are very close, we can say that no indications of overfitting. Both are showing similar levels of accuracy. Now, we bring the confusion matrix to inspect other performance matrix. And these are the values of true positives, true negatives, false positives and false negatives.\n",
      "Now, the concern is false positive is high. That means 1408 false positives and that is not far off from the true negatives. That is a concern for us. We visualize the confusion matrix with the seaborn heatmap. And if you look at the true positive rate, it is 5999 which is good. And if you look at the true negative rate, it is 1897. That's not good enough when we look at the false negatives.\n",
      "So, that is a concern for us. Now, let us inspect the classification report. The precision for less than or equal to 50K income 0.93, recall 0.81, f1-score 0.86. All these values are very good. But whereas the precision for greater than 50k income, it is only 0.57 and f1-score is only 0.67. So, those two Matrix are of little concern to us. Then accuracy is 0.81, overall accuracy. That is good. Now, we are printing each of these scores individually. So, precision is 0.8099.\n",
      "That means it is 80.99%. Recall 0.9281. Then we have the true positive rate, false positive rate, specificity, classification error. And we are computing the ROC AUC score which is\n",
      "14\n",
      "important for us, and it is 0.8065. Any value more than 0.7 is good. i This is more than 0.8. So, that is pretty good. So, we have a good model for classifying. So, we have a good model for predicting whether a person can make more than 50k per annum.\n",
      "Video 13: Supervised Learning Techniques: Summary\n",
      "This video summarises the module on supervised learning. In this module, we have discussed the context of machine learning, then the concepts of supervised learning with some examples, then the different types of classification task, which is coming under supervised learning. Then the techniques for data collection, then preprocessing methods, and the encoding techniques for data coding.\n",
      "Then the performance metrics of the classifiers are discussed, and we have demonstrated the support vector machines and Naive Bayes classifier also.\n"
     ]
    }
   ],
   "source": [
    "print(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER=5\n",
    "SUBJECT='Machine Learning'\n",
    "TONE='Complex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Video 1: Supervised Learning Techniques: Overview\n",
      "Hello everyone. Welcome to the module on supervised learning techniques. Machine Learning typically use historical data to build models. Supervised learning is used for solving classification or prediction problems with the help of labelled data. The data collected from a source or from different sources may not be directly usable for building machine learning models.\n",
      "So, this data needs to undergo pre-processing which involves multiple steps including data cleaning, data integration, data reduction and so on. The performance of ML classifier is evaluated using a number of metrics such as accuracy, precision, recall and so on. This module demonstrates the classification techniques in machine learning with the help of two baseline classifiers such as SVM classifier and Naive Bayes classifier.\n",
      "Video 2: The Context of Machine Learning\n",
      "AI/ML is one of the most important strategic technologies now. For most of the industries, AI/ML is part of their business strategy, IT strategy or both. Businesses and IT professionals are finding amazing use cases for AI/ML and already experiencing its benefits, but still a lot more to do to realise the potential of AI/ML. Positive impacts of AI/ML are reported from businesses on brand awareness, revenue generation and expense reduction. More and more companies are increasing their overall AI/ML budgets.\n",
      "IT departments generally have a strong understanding of AI/ML, but the leadership or other departments do not understand it well. Thus, there is a need to make the stakeholders understand AI/ML better. The current uses of AI/ML in management include optimisation of the processes to improve their speed and efficiency, understanding the employees better, personalising the content to the customers and understanding the customers better, increasing the revenues, gaining competitive advantage, predicting business performance and industry trends, understanding market effectiveness and reducing the risk.\n",
      "Video 3: Supervised Learning\n",
      "Let us start with Supervised Learning. The supervised learning approach is similar to the human learning under the supervision of a teacher. The teacher provides good examples to the student to understand and then the student derives general rules from the examples. In\n",
      "2\n",
      "supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs.\n",
      "Consider an example of fruits, where we train the ML model using the data, which is labelled for classification as apples and oranges. Once the training is complete, the machine is ready to classify new fruits data without label. Look at some examples of supervised learning classification, we have a label dataset of images of cats and dogs. Now, if we have a model to identify if the next image is a dog or a cat.\n",
      "This is an example for binary classification. We have a set of movie reviews with sentiment label as good, neutral or bad. Now, identify a new review sentiment as good, neutral or bad. This is an example for multiclass classification. We have images of handwritten digits from 0 to 9. Now, identify a number from hand-drawn digits image. This is an example for multiclass classification.\n",
      "The supervised learning process. We have a basket filled with two kinds of fruits, say apple and banana. The first step is to train the model with all different fruits one by one using two features: The first feature is shape; rounded with the depression at the top. The second feature is Color; red. If it is rounded with the depression at the top and the color is red, then it is apple. If the shape is long curving cylinder and the color is green or yellowgreen color, then it is banana.\n",
      "Now, the model is ready to classify a new fruit. It will classify the fruit using its shape and color as banana or apple. Thus the machine learns from training data, which is the basket containing fruits, and then applies this knowledge to test new data which could be in the new basket.\n",
      "Another supervised task is to predict a target numeric value such as the price of a used car; given the set of features such as mileage, age, brand and so on. These features are called Predictors. This sort of task is called Regression. Here to train the model, we need to give it many examples of cars, including their predictors and the labels. Here, the labels are the prices of these cars.\n",
      "Now, let us look at the types of supervised learning. Supervised learning techniques can be classified into two: classification and prediction. The purpose of classification is to predict the class of objects whose class label is unknown. Here we are categorising the new incoming data based on our assumptions and the data we have with us. In prediction, we predict the data based on the previous data we have with us and based on the assumptions. For example, we can forecast the likelihood of a particular outcome, such as whether or not a customer will churn in 30 days.\n",
      "Video 4: Classification Tasks\n",
      "So, in this video, we'll be talking about the classification class. There are four types of classification tasks that we may encounter in real life. These are binary classification, multiclass classification, multi-label classification and imbalanced classification. Binary classification is the task when we have two class labels.\n",
      "3\n",
      "Example, cancer detection, cancer patient or not. Conversion prediction, buy or not. Email spam detection, spam or harm. Binary classification tasks typically involve a normal state and an abnormal state which are the two classes. For example, not spam is the normal state and spam is the abnormal state. Similarly, cancer not detected is the normal state and cancer detected is the abnormal state. The class for the normal state can be assigned a class label one and the class with abnormal state can be assigned a class label zero.\n",
      "The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome either as a zero or one. We can model a binary classification task with a model that predicts a Bernoulli probability distribution for each example. This means that the model predicts the probability of an example belonging to the abnormal state. Popular binary classification algorithms include naive bayes, support-vector machine, logistic regression, K-nearest neighbours and decision trees.\n",
      "The logistic regression and support-vector machines do not support more than two classes. Now, let us look at the multiclass classification. Multiclass classification involves classification tasks that have more than two class labels. Examples include optical character recognition, face recognition, plant species classification. In multiclass classification, there is no notion of normal and abnormal outcomes.\n",
      "Here is an example, which is classified as belonging to one among a range of non classes. In face recognition, the model may be predicting a photo as belonging to one among thousands of faces in the face recognition system. Predicting a sequence of words in text translation models is a type of multiclass classification. Here, the size of the vocabulary defines the number of classes that may be predicted and could be many thousands of words in size.\n",
      "A multiclass classification task can be modelled with a model that predicts a multinolli probability distribution for each example. This means that the multiclass classification model predicts the probability of an example belonging to each class label. The popular multiclass classification algorithms include naive bayes, K-nearest neighbours, decision trees, random forest and gradient boosting. The third type is the multi-label classification.\n",
      "In multi-label classification, there will be two or more class labels where one or more class labels may be predicted for each case. For example, in image classification, a given image may have multiple objects in a scene and the multi-label model may predict the presence of multiple known objects in the image such as bike, car, person and so on. In binary classification and multiclass classification, only a single class label is predicted in each case.\n",
      "We can model a multi-label classification task with a model that predicts multiple outputs with each output prediction coming from a Bernoulli probability distribution. Multi-label versions of the standard classification algorithms are needed for multi-label classification, such as multi-label decision trees, multi-label gradient boosting, multi-label random forest. In imbalanced classification, the number of examples in each class is imbalanced or unequally distributed.\n",
      "Typically, they are binary classification tasks. The majority of the examples in the training data set will belong to the normal class and only a minority of the examples will be in the abnormal class. For example, brain tumor detection or financial fraud detection or outlier\n",
      "4\n",
      "detection. In each of this case, we can see that the abnormal state will be less in number. The imbalanced classifiers are modelled as binary classification task with specialised techniques.\n",
      "These specialised techniques will change the composition of the samples in the training data set by undersampling the majority class, or oversampling the minority class. Undersampling would decrease the proportion of the majority class until the number matches to the minority class. Oversampling would resemble the minority class proportion to match the majority class proportion.\n",
      "Video 5: Data Collection\n",
      "In this video, we'll discuss the data collection techniques. Data collection is the process of gathering data from different sources. To be useful for machine learning, this data must be collected and stored in a way that makes sense for the business problem at hand. So, the data has to be relevant.\n",
      "Humans and machines are generating lots of data. This data include numeric data, categorical data or text data. We can use primary or secondary data for building machine learning models. The primary data sources include surveys, observations, questionnaires, experiments, personal interviews and so on. The secondary data sources can be public data sets, research reports, government publications, websites, publications from independent research labs and so on. How to collect data if we don't have any primary data with us?\n",
      "The possibilities are rely on open source data sets, Google data set search, Microsoft research open data, Amazon data sets, Kaggle data sets, UCI data sets where UCI stands for University of California at Irvine. Public data sets include mall customer data set, which contains information about people visiting the mall in a particular city. Another public data set is MNIST, Modified National Institute of Standards and Technology data set, which contains handwritten digits from 0-9.\n",
      "Another public data set is the Boston Housing data set, which contains data collected by the US census service concerning housing in the Boston area. Other public data sets include: Fake News Detection data set, Wine quality data set, SOCR which stands for Statistics online computational resource data which is about the heights and weights of people, then credit card fraud detection data set and COVID-19 data set.\n",
      "Video 6: Data Preprocessing\n",
      "This video explains you data preprocessing. There could be a lot of data for solving a problem, but the data available could be dirty. So, we cannot apply this data to our machine learning algorithms directly. If we use dirty data to develop the machine learning model, then the outcome will also be dirty. There is a saying, \"Garbage in, garbage out\", which will be true with respect to data in machine learning models.\n",
      "5\n",
      "So, there is a need to have a data preprocessing in every machine learning solution, so that the correct data is supplied to the model. This is a necessary requirement but not sufficient to have a great model. The machine learning model outcome can be bad with good data also if we choose the wrong machine learning model. So, we need to have both good data as well as the right machine learning model for a particular problem. Data preprocessing is divided into four stages: Data cleaning, Data integration, Data reduction, and Data transformation.\n",
      "Data cleaning is the process of identifying or removing incorrect, corrupted, incorrectly formatted, duplicated or incomplete data from a data set. Data integration is the process of bringing together data from different sources, both traditional and non-traditional into a meaningful data set for analytics.\n",
      "Data reduction techniques are used to preserve data in a reduced or condensed form without loss of information. Data transformation is the process of converting the raw data into a format that would be suitable for building a machine learning model. Let us begin with data cleaning step. The number of steps in data cleaning depends on the cleanliness of the incoming data.\n",
      "Data cleansing involves detecting and correcting corrupt or inaccurate records from a set of records or table. So, there has to be mechanisms to identify incomplete, incorrect, inaccurate or irrelevant parts of the data and then replace, modify or delete the dirty data.\n",
      "Data imputation is a technique use for replacing the missing data with some substitute value to retain most of the data in the data set. This is needed because removing the data from the data set every time is not feasible and can lead to a reduction in the data size, which may cause concerns for biases.\n",
      "The approaches to data imputation are the following: The first thing is do nothing. Some of the algorithms automatically takes care of this missing data. The second possibility is imputation using the median or mean values. A third technique is imputation using the most frequent values. Another technique is using zero or constant values. Then, the possibilities are using algorithms like K-nearest neighbours or the use of multi-variant imputation by chained equation which is known as MICE and also imputation using deep learning.\n",
      "Now let us look at data integration techniques. Here data integration solution integrates the data collected from multiple sources which could be on-premise or from cloud sources. The data integration should address the security, privacy and data quality issues also. It uses techniques such as ETL and ELT. ETL stands for extract, transform and loading data from multiple data sources to a single data store, which is then loaded into the data warehouse.\n",
      "ELT stands for extract and load raw data from multiple data sources into a target data lake or cloud data warehouse where the data can be transformed when needed. ELT is more appropriate for machine learning applications. Data reduction is the transformation of data obtained empirically or experimentally into an optimal or simplified form for analysis. Three types of data reduction exist which include feature reduction, case reduction and value reduction.\n",
      "Feature reduction reduces the number of columns, case reduction reduces the number of rules whereas value reduction reduces the numeric value. More input features or columns\n",
      "6\n",
      "make a predictive modelling task more challenging to model and is generally referred to as the curse of dimensionality.\n",
      "Principal component analysis, PCA, Factor analysis, FA, Linear discriminate analysis, LDA and Truncated singular value decomposition, SVD, are examples of feature reduction techniques. Next is data transformation. Data transformation is the process of converting the raw data into a format or structure that would be suitable for machine learning model building.\n",
      "This may include data smoothing, data aggregation, discretisation, generalisation, attribute construction and normalisation. Data smoothing is used for removing the noise from a data set. Noise is the distorted and meaningless data within a data set.\n",
      "Data aggregation is the process of collecting data from multiple sources and storing it using a single format. Data discretisation is a process of converting continuous data into a set of data intervals. This may make the data analysis easier. Data generalisation is the process where low-level data attributes are transformed into high-level data attributes using concept hierarchies.\n",
      "This is useful to get better insights about the data. For example, age data in a data set can be transformed into categorical values such as young, middle-aged, old and so on. Attribution construction allows new attribute creation from the existing set of attributes.\n",
      "For example, the attributes such as employee name, employee ID and address from one data set can be used to construct another data set that contains information about the employees who joined in a particular year, for instance, 2021. Data normalisation transforms the data, so that all data falls within a given range. The popular data normalisation methods include: Min-max normalisation, decimal scaling and Z-score normalisation.\n",
      "Video 7: Data Encoding\n",
      "This video explains you techniques for data encoding. Machine learning algorithms can understand only numbers. So, for handling text, the text or string columns must be converted into numerical columns. For example, if the gender column has values like male or female, we need to map them to numerical values.\n",
      "This conversion process is called encoding. We can use One-hot encoding and Label encoding which is available in scikit-learn for encoding categorical columns into numerical columns. Let us look at label encoding first. Label encoding is simple when we assign a numerical value to each value in the categorical column value. Each value in the categorical column is called label. For example, gender column can be labelled as zero for female and one for male. This means that we can simply replace male by one and female by zero.\n",
      "But if you want to assign labels to the continents of the world, it needs seven values. That means from zero to six. This may end up giving more weightage to particular values when we compare these numerical values. This is a drawback of label encoding. Now let us look at one-hot encoding.\n",
      "This is called one-hot encoding because only one index has a non-zero value. The value one is called 'hot' and the value zero is called 'cold'. When we apply one hot encoding to\n",
      "7\n",
      "represent the seven continents in the world, we will have a seven-bit vector. In this vector only one of the seven bits can be a numeric value one at a time which corresponds to a continent. For example, Africa being first in the lexicographic ordering can be coded as one followed by six zeroes, whereas South America being the last in the lexicographic ordering can be coded as six zeroes followed by one.\n",
      "Video 8: Performance Metrics of Classifiers\n",
      "This video explains you the performance metrics of classifiers classification is the process of predicting the class of given data points. There are two types of learners in classification, lazy learners and eager learners. Lazy learners simply store the training data and wait until a testing data appears. When the data appears.\n",
      "Classification is done based on the most related data in the store training data. Lazy learners need less training time but takes more time in predicting. An example is the K-nearest neighbour classifier. Eager learners construct a classification model based on a given training data before receiving the data for classification, eager learners take a long time for training and less time to predict.\n",
      "Examples include decision tree bayes and artificial neural network classifiers. Many classification algorithms are available now but it is not possible to conclude which one is superior to the other. The suitability of an algorithm depends on the application and nature of the available data set. For evaluating a classifier we have two prominent methods. These are the holdout method and the cross- validation method. In the holdout method, the given data set is divided into two partitions as test and training data sets.\n",
      "This could be 20:80 ratio or 30:70 ratio, or it could be 90:10 ratio also depending on the size of the data available, we can have other combinations. Also the train set will be used to train the model and the unseen test data will be used to test the models predictive power. In cross- validation, the data cities randomly partitioned indu que mutually exclusive subsets each approximately of equal size and one is kept for testing while others are used for training this process is iterated throughout the whole K-fold. K-fold Cross validation in general eliminates the overfitting issues. Overfitting occurs when a statistical model fits exactly against its training data. When this happens, the algorithm cannot perform accurately against unseen data.\n",
      "The sign of overfitting is when air around testing set is much greater than the air around training set. Under fitting happens when the model cannot capture the underlying trend of the data. It occurs when the model is too simple. The sign of under fitting is error on both training and test data. The performance of a classification model can p assist using accuracy, which is the percentage of correct predictions. However accuracy depends on the data. For example, considering binary classification model using a logistic regression with an accuracy of 95%.\n",
      "8\n",
      "This model should be very good But the data was imbalanced where 95% of the records for Class zero and 5% records of Class one and the model was a poor model And was crediting zero for all the cases but we get accuracy is 95%. This is not correct. So we need to identify other evaluation metrics also depending upon the data and problem at hand. Another metric for classifications is the arrow sickle which overcomes some of the problems faced with accuracy. R. O. C. Stands for receiver operator characteristics which is a probability curve that has a false positive rate fpr at the X axis and the true positive rate T P R at the Y axis.\n",
      "It is primarily used for binary classification model evaluation. When Area under the Curve, a U. C. is one. Then the classifier is able to perfectly distinguish between all the positive and negative class points correctly. When a U.C. is between .5 and one there is a high chance that the classifier will be able to distinguish the positive class values from the negative class values. When you see is exactly .5. Then the classifier is not able to distinguish between positive and negative class points. If you see zero then the classifier would be predicting all negatives as positives and all positives as negatives. Four types of outcomes are possible while evaluating the performance of a classification model.\n",
      "True positives, tp true negatives, tiene false positives FP And false negatives or false positives is also known as Type one error. False negatives is also known as Type two error. A confusion matrix is a tool for summarizing the performance of a classification algorithm. It gives us a clear picture of the performance of a classification model and the types of errors it produces. It is giving the correct and incorrect predictions broken down by each category. The summary is represented in a tabular form for evaluating machine learning classifiers.\n",
      "The following five criteria are used more commonly classification accuracy, precision recall, a phone score and area under the R. O. Seeker. The precision score represents the model stability to currently predict the positives out of all the positive predictions it makes procession is calculated us true positive divided by some of false positives and true positives. Recall measures how good our Ml model is at identifying all actual positives out of all positives that exist within their data set. Recall is calculated by dividing true positive rates by the sum of false negatives and true positives.\n",
      "The accuracy tells us how often we can expect our Ml model will correctly predict an outcome out of the total number of times it makes predictions, accuracy is calculated by dividing the sum of true positives and true negatives by the sum of true positives, false negatives. True negatives and false positives. Their phone score is used as a single value that provides high level information about the model's output quality. A phone score is calculated bye Dividing the product of precision and recall by the sum of precision and recall multiple. eight x 2\n",
      "9\n",
      "Video 9: SVM Classifier\n",
      "This video explains you the SVM classifiers. A Support Vector Machine or SVM, is a supervised machine learning algorithm which can be used for both classification and regression problems, but majorly used for classification.\n",
      "Support vector machine constructs a line or a hyperplane in a high or infinite dimensional space, which is used for classification, regression, or other tasks like outlier detection. Support vector machine makes sure that the data is separated with the widest margin. Support vectors are data points that are close to the hyperplane that determines the orientation and position of the hyperplane. Support vector machines also draws two parallel hyperplanes called marginal planes to this hyperplane, such that they touch the nearest positive and negative points. The distance between the main hyperplane and the two parallel hyperplanes is called marginal distance.\n",
      "The marginal planes help in generalising the model, preventing overfitting. The goal of support vector machine is to train a model that assigns new unseen objects into a particular category. Support vector machines does this classification by creating a linear partition of the feature space into two categories based on the features in the new unseen objects, it places an object above or below the separation plane leading to a categorisation. Support vector machines is a non-probabilistic linear classifier because the features in the new objects fully determine its location in feature space and there is no stochastic element involved.\n",
      "Video 10: SVM Classifier: Demonstration\n",
      "This video is a demonstration of SVM classifier. So, what we do is we are trying to predict diabetes using a data set that has some predictor variables. So, what we are doing is we are trying to predict diabetes of patients using features like BP, BMI, insulin level, skin thickness and so on. So, we begin with importing the necessary libraries.\n",
      "You can see that Pandas, NumPy, Matplotlib, Seaborn. So all these libraries are imported, scikitk-learn also. This is the Pima Indians Diabetes Database. Using the read_csv command, we are reading the file data sets_228_482_diabetes. Now, we look at the sample data. So, you can see that these are the columns. Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, Age and Outcome. So, Outcome is the prediction or the label as diabetic patient or not.\n",
      "Now, we use the describe() command to get the descriptive statistics of the data, and you will get count, mean, std, min then the quartiles and max. And the size of the data 768 rows and 9 columns. We are checking the missing values using this statement. Our data frame is df.isnull() and we are getting false. That means there are no null values. Now, we don't want zero values.\n",
      "So, replace all zeros by not a number. Then splitting data into tests and train data sets and the test_size is 0.2, that means 20% test size and 80% training size. Now, we use the support vector machine. We fit the model, and now we check the accuracy of the model. And we get\n",
      "10\n",
      "an accuracy of 0.79. Now, let us look at the confusion_matrix. Because accuracy alone won't be a good measure, so we look at other performance metrics as well.\n",
      "So, we have the Confusion matrix and the True Positives, True Negatives,\n",
      "False Positives and False Negatives are here. And we see that False Positives is good but False Negatives is not good because you have 23 that and True Negatives is only 24. Now, we use the Seaborn to display the confusion matrix.\n",
      "Now, look at the confusion matrix. We can see that the true positive rate is 98, and true negatives is 24. The predicted negative where it is actually positive. That means this is false negative which is 23 and the false positive is nine. So, since the false negative is 23 and true negative is only 24, the confusion matrix shows that this model is not very good. Now, you look at the classification_report, the precision, recall, f1-score and accuracy score. So, you can see that if you look at for 0, that means normal condition, precision, recall and f1-score are good.\n",
      "But whereas, the abnormal condition, that is diabetic condition, precision is only 0.73, recall is 0.51 and f1-score is 0.60. Accuracy is 0.79. So, recall and f1_score are not very good for diabetic condition. Now, we are computing each of this metric individually. So, Classification accuracy, the exact value is 0.7922. The Classification error is calculated which is 0.2078, the Precision is 0.9159, Recall 0.8099, the True Positive Rate is 0.8099, False Positive 0.2727, Specificity 0.7273 and the ROC AUC score is computed as 0.7133. Anything more than 0.7 is good, so this is a reasonably good value.\n",
      "Video 11: Naïve Bayes Classifier\n",
      "This video explains you the Naive Bayes classifier. Naive Bayes classifier is based on the Bayes' theorem. It assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. It is called Naive because the above assumption may or may not be true. Naive Bayes classifier is simple and very effective in building fast machine learning models for classification. Bayes' theorem gives the probability of an event occurring, given the probability of another event that has already occurred.\n",
      "So, it can be mathematically stated as: P(A|B)= (P(B|A)*P(A))/P(B), where A and B are events and P(B) is not equal to zero. P(A) is the prior probability, that is, the probability of the event before evidence is seen. And, P(A|B) is a posterior probability of B, that is, probability of event after the evidence is seen. So, we have an example. Event A: Diagnosed as a liver patient and Event B: Diagnosed as alcoholic.\n",
      "Now, we have patient data where 10% of patients are coming for treatment are having liver disease, that is, P(A) is 0.1. And, 5% of the patients coming for treatment are alcoholic, that means P(B) is 0.05.\n",
      "11\n",
      "Also those who have liver disease, it is known that 7% are alcoholic, that means P(B|A) is 7%. Now by applying Bayes' theorem, we can calculate the P(A|B), which is (0.07* 0.1)/0.05, which is equal to 0.14. That is, among the alcoholics, the chances of having liver disease is 0.14, that means 14%. This is more than the 10% suggested by the existing patient data. Types of Naive Bayes classifiers.\n",
      "The first one is multinomial Naive Bayes. The features and predictors used for document classification are the frequency of the words present in the document. The second type is Bernoulli Naive Bayes. Here, the predictors are Boolean variables, such as taking values 'yes' or 'no'. So, if used for document classification, it says a word occurs in the text or not. The next is Gaussian Naive Bayes. Here, the predictors take up continuous values.\n",
      "Video 12: Naïve Bayes Classifier: Demonstration\n",
      "This video is a demonstration of the Naive Bayes Classifier. We are building a Naive Bayes Classifier to predict whether a person makes over 50k per annum or not. So, our file is async2_naive-bayes-classifier. We begin with importing the files. Now, we are importing the data set, that is, Adult csv. We are importing the file adult.csv.\n",
      "So, this is an adult income data set. It's a CSV file, read it into a DataFrame df. Now, we make a exploratory data analysis first looking at the dimensions of the data set using the .shape. So, this gives that there are 32,561 rows for this data and 15 column. Now, let us look at the top five rows of this data set. And you can see that the columns are not labelled.\n",
      "So, we need to rename the columns. So, these are the columns: age, workclass, functional weight, education and many more. So, all these are assigned as column names. Now, we print the columns for this DataFrame and once again look at the DataFrame after assigning the columns. Now, the data set has a decent look. So, this is also our data cleaning process. Now, we get a summary of the data set. We have 15 features here and showing that all these 32,561 are non-null.\n",
      "There are different data types, integer, object data types. Now, let us look at the categorical variables. There are nine categorical variables, and we'll look at the first 5 rows with only the categorical variables and these are the ones. So, the last column is income where it is less than or equal to 50k. So, what it gives is there are nine categorical variables, and these are the names, and income is the target variable. Now, we check for missing values.\n",
      "So, it says that there are no missing values. Then we check the frequency counts. So, each of this categorical variable, you can see the frequency count and you can see a question mark also here. So, the percentage-wise distribution we can see. So here, what we see that several variables have missing values. Generally, these missing values are coming with NaN code, but here it is not NaN.\n",
      "The missing values are coded as question mark. That's why our check was giving that there are no missing values. So, you can see that at multiple places you have the question marks. So, all those are missing values. Now, let us explore the workclass variable.\n",
      "12\n",
      "So, the unique variables are this and you see the question mark also here. Now, let us look at the frequency distribution of the values in this workclass variable and the question mark, there are 1,836 positions. These are to be replaced with NaN. Now again we are checking the frequency distribution. Now, we see that the question mark is not there. Now, we explore the occupation and native_country columns also.\n",
      "So, these are the unique variables in that occupation. Then check out the value counts. Here also, you see the question mark, 1,843. Replace it with NaN. Onc again, you check, now it's not there. Similarly, for the native_country variable. native_country, you have multiple countries, from United States to Holland-Netherlands. Frequency distribution. So here, the question mark there are 583. Replacing with NaN. Now, the question mark is not there.\n",
      "So, once again, checking for missing values in the categorical variables. Now, we see that all their NaNs are coming as missing values. So, workclass, we have replaced 1,836 question marks to NaN, occupation, 1,843 and native_country, 583. All these are now appearing us null.\n",
      "Now, we need to explore the numerical variables also. So, these are the numerical variables, and we explore them. Now, check for missing values. So, no missing values in that. Now, we are separating out the target variable that is income. So, this is assigned to X, And income alone is Y. Now, we declare the feature vector and target variable. The X variables are by dropping the income, and Y is the income variable. Now, we split the data into separate training and test data set. So, we use the Scikit-learn train_test_split.\n",
      "Now, you see that the test size is 0.3, that means 30% test data and 70% training data. Now, we are checking the shape of both training and test data. So, we can see that the training data has 22,792 rows and, of course, 14 columns, whereas test data has 9,769 rows. Now we check the data types in x_train. These are the data types. And we display the categorical variables. These are the categorical variables, and these are the numerical variables. Now, we check for the missing values in the categorical variables in the training data.\n",
      "So, these are the percentage of missing values. You can see that in workclass, occupation, and native_country there are missing values. Now, we are printing the categorical variables with missing values. And we are going to impute the missing categorical variables. We are going to drop them but rather we impute them with the most frequent values from the data set. So, for each of the variables workclass, occupation, native_country, we fill with the most frequent value from the data set. Now, we are checking again the missing values. We see that nothing is left out. So, no missing values now.\n",
      "So, test data, we are checking for the missing values. So, there are no missing values. So, checking missing values in x_train, no missing values. x_test, no missing values. Once we verify that there are no missing values in the training and test data, now, we can encode the categorical variables.\n",
      "13\n",
      "So, these are the categorical variables. And we display the first five rows of that. We need to install the category_encoders using the pip install command, pip install category_encoders. If we don't do it, we'll end up in an error. So, we'll install it then import the category_encoders as ce. Then we go for the one-hot encoding of the category variables. We have five rows and 105 columns.\n",
      "Now, check the shape of the training data. Now, you see that there are 22,792 rows 105 columns after one hot encoding. Now, we are checking the first five rows of the test data. Five rows and 105 columns. Now, we are checking the first five rows of the test data. There are 9,769 rows 105 columns. So, both training and test data are ready for modern building. Now, we need to bring all the feature variables to the same scale. This is called feature scaling.\n",
      "So, we import the RobustScalar. Now, after applying the scaler, we see that the values are brought down to the same scale. If you look at the previous values of these variables, you can see that age, numerical values and a fnlweight. Also, you can see the numerical values. Once you scale it, all those are brought to the same scale.\n",
      "So, after scaling all these values are brought to the same scale. That is between zero and one for absolute values. Now, we'll train the Gaussian Naive Bayes classifier using the training data set. So, training is complete. Now we use the test data set for prediction. So, after testing, we check the accuracy score.\n",
      "Accuracy score is 0.8083. That is pretty good. And we need to check the training set accuracy score that is also around 80%, 0.8067. So, since both training accuracy and test accuracy are very close, we can say that no indications of overfitting. Both are showing similar levels of accuracy. Now, we bring the confusion matrix to inspect other performance matrix. And these are the values of true positives, true negatives, false positives and false negatives.\n",
      "Now, the concern is false positive is high. That means 1408 false positives and that is not far off from the true negatives. That is a concern for us. We visualize the confusion matrix with the seaborn heatmap. And if you look at the true positive rate, it is 5999 which is good. And if you look at the true negative rate, it is 1897. That's not good enough when we look at the false negatives.\n",
      "So, that is a concern for us. Now, let us inspect the classification report. The precision for less than or equal to 50K income 0.93, recall 0.81, f1-score 0.86. All these values are very good. But whereas the precision for greater than 50k income, it is only 0.57 and f1-score is only 0.67. So, those two Matrix are of little concern to us. Then accuracy is 0.81, overall accuracy. That is good. Now, we are printing each of these scores individually. So, precision is 0.8099.\n",
      "That means it is 80.99%. Recall 0.9281. Then we have the true positive rate, false positive rate, specificity, classification error. And we are computing the ROC AUC score which is\n",
      "14\n",
      "important for us, and it is 0.8065. Any value more than 0.7 is good. i This is more than 0.8. So, that is pretty good. So, we have a good model for classifying. So, we have a good model for predicting whether a person can make more than 50k per annum.\n",
      "Video 13: Supervised Learning Techniques: Summary\n",
      "This video summarises the module on supervised learning. In this module, we have discussed the context of machine learning, then the concepts of supervised learning with some examples, then the different types of classification task, which is coming under supervised learning. Then the techniques for data collection, then preprocessing methods, and the encoding techniques for data coding.\n",
      "Then the performance metrics of the classifiers are discussed, and we have demonstrated the support vector machines and Naive Bayes classifier also.\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 5 multiple choice questions for Machine Learning students in Complex tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 5 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-5Gjws46t5yzh8hy5ZkgcX1ir on tokens per min (TPM): Limit 10000, Requested 10355. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[57], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_openai_callback() \u001b[38;5;28;01mas\u001b[39;00m cb:\n\u001b[1;32m----> 2\u001b[0m     response\u001b[38;5;241m=\u001b[39m\u001b[43mgenerate_evaluate_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mText\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnumber\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mNUMBER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mSUBJECT\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtone\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTONE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRESPONSE_JSON\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    379\u001b[0m }\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\sequential.py:104\u001b[0m, in \u001b[0;36mSequentialChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, chain \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchains):\n\u001b[0;32m    103\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m _run_manager\u001b[38;5;241m.\u001b[39mget_child()\n\u001b[1;32m--> 104\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mchain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mknown_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m     known_values\u001b[38;5;241m.\u001b[39mupdate(outputs)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {k: known_values[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_variables}\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:180\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    179\u001b[0m     emit_warning()\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\base.py:381\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \n\u001b[0;32m    351\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    374\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    377\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    379\u001b[0m }\n\u001b[1;32m--> 381\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\base.py:164\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    163\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    165\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\base.py:154\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    153\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 154\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    155\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    157\u001b[0m     )\n\u001b[0;32m    159\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    160\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\llm.py:126\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    123\u001b[0m     inputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any],\n\u001b[0;32m    124\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    125\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m--> 126\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain\\chains\\llm.py:138\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list, run_manager)\u001b[0m\n\u001b[0;32m    136\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    145\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[0;32m    146\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[0;32m    147\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:777\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    769\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    770\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    771\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    775\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    776\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 777\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:634\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[0;32m    633\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 634\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    635\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    636\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    637\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    638\u001b[0m ]\n\u001b[0;32m    639\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:624\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[0;32m    622\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    623\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 624\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    630\u001b[0m         )\n\u001b[0;32m    631\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:846\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 846\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    849\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    850\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:473\u001b[0m, in \u001b[0;36mChatOpenAI._generate\u001b[1;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[0;32m    467\u001b[0m message_dicts, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_message_dicts(messages, stop)\n\u001b[0;32m    468\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    469\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    470\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: stream} \u001b[38;5;28;01mif\u001b[39;00m stream \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}),\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    472\u001b[0m }\n\u001b[1;32m--> 473\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\langchain_community\\chat_models\\openai.py:384\u001b[0m, in \u001b[0;36mChatOpenAI.completion_with_retry\u001b[1;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_openai_v1():\n\u001b[1;32m--> 384\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m retry_decorator \u001b[38;5;241m=\u001b[39m _create_retry_decorator(\u001b[38;5;28mself\u001b[39m, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[0;32m    388\u001b[0m \u001b[38;5;129m@retry_decorator\u001b[39m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_completion_with_retry\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_utils\\_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\resources\\chat\\completions.py:815\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    812\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[0;32m    813\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m    814\u001b[0m     validate_response_format(response_format)\n\u001b[1;32m--> 815\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    818\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    819\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    820\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    821\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:1277\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1265\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1272\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1274\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1275\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1276\u001b[0m     )\n\u001b[1;32m-> 1277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:954\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    952\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 954\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    955\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:1014\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1011\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered Exception\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1023\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaising connection error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m APIConnectionError(request\u001b[38;5;241m=\u001b[39mrequest) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1041\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_should_retry(err\u001b[38;5;241m.\u001b[39mresponse):\n\u001b[0;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[39;00m\n\u001b[0;32m   1089\u001b[0m \u001b[38;5;66;03m# different thread if necessary.\u001b[39;00m\n\u001b[0;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\91897\\mcqgen\\env\\lib\\site-packages\\openai\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1055\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Request too large for gpt-4 in organization org-5Gjws46t5yzh8hy5ZkgcX1ir on tokens per min (TPM): Limit 10000, Requested 10355. The input or output tokens must be reduced in order to run successfully. Visit https://platform.openai.com/account/rate-limits to learn more.', 'type': 'tokens', 'param': None, 'code': 'rate_limit_exceeded'}}"
     ]
    }
   ],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "            \"text\": Text,\n",
    "            \"number\": NUMBER,\n",
    "            \"subject\":SUBJECT,\n",
    "            \"tone\": TONE,\n",
    "            \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:0\n",
      "Prompt Tokens:0\n",
      "Completion Tokens:0\n",
      "Total Cost:0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tokens:{cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "print(f\"Total Cost:{cb.total_cost}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
