Video 1: Supervised Learning Techniques: Overview
Hello everyone. Welcome to the module on supervised learning techniques. Machine Learning typically use historical data to build models. Supervised learning is used for solving classification or prediction problems with the help of labelled data. The data collected from a source or from different sources may not be directly usable for building machine learning models.
So, this data needs to undergo pre-processing which involves multiple steps including data cleaning, data integration, data reduction and so on. The performance of ML classifier is evaluated using a number of metrics such as accuracy, precision, recall and so on. This module demonstrates the classification techniques in machine learning with the help of two baseline classifiers such as SVM classifier and Naive Bayes classifier.
Video 2: The Context of Machine Learning
AI/ML is one of the most important strategic technologies now. For most of the industries, AI/ML is part of their business strategy, IT strategy or both. Businesses and IT professionals are finding amazing use cases for AI/ML and already experiencing its benefits, but still a lot more to do to realise the potential of AI/ML. Positive impacts of AI/ML are reported from businesses on brand awareness, revenue generation and expense reduction. More and more companies are increasing their overall AI/ML budgets.
IT departments generally have a strong understanding of AI/ML, but the leadership or other departments do not understand it well. Thus, there is a need to make the stakeholders understand AI/ML better. The current uses of AI/ML in management include optimisation of the processes to improve their speed and efficiency, understanding the employees better, personalising the content to the customers and understanding the customers better, increasing the revenues, gaining competitive advantage, predicting business performance and industry trends, understanding market effectiveness and reducing the risk.
Video 3: Supervised Learning
Let us start with Supervised Learning. The supervised learning approach is similar to the human learning under the supervision of a teacher. The teacher provides good examples to the student to understand and then the student derives general rules from the examples. In
2
supervised learning, the algorithm builds a mathematical model from a set of data that contains both the inputs and the desired outputs.
Consider an example of fruits, where we train the ML model using the data, which is labelled for classification as apples and oranges. Once the training is complete, the machine is ready to classify new fruits data without label. Look at some examples of supervised learning classification, we have a label dataset of images of cats and dogs. Now, if we have a model to identify if the next image is a dog or a cat.
This is an example for binary classification. We have a set of movie reviews with sentiment label as good, neutral or bad. Now, identify a new review sentiment as good, neutral or bad. This is an example for multiclass classification. We have images of handwritten digits from 0 to 9. Now, identify a number from hand-drawn digits image. This is an example for multiclass classification.
The supervised learning process. We have a basket filled with two kinds of fruits, say apple and banana. The first step is to train the model with all different fruits one by one using two features: The first feature is shape; rounded with the depression at the top. The second feature is Color; red. If it is rounded with the depression at the top and the color is red, then it is apple. If the shape is long curving cylinder and the color is green or yellowgreen color, then it is banana.
Now, the model is ready to classify a new fruit. It will classify the fruit using its shape and color as banana or apple. Thus the machine learns from training data, which is the basket containing fruits, and then applies this knowledge to test new data which could be in the new basket.
Another supervised task is to predict a target numeric value such as the price of a used car; given the set of features such as mileage, age, brand and so on. These features are called Predictors. This sort of task is called Regression. Here to train the model, we need to give it many examples of cars, including their predictors and the labels. Here, the labels are the prices of these cars.
Now, let us look at the types of supervised learning. Supervised learning techniques can be classified into two: classification and prediction. The purpose of classification is to predict the class of objects whose class label is unknown. Here we are categorising the new incoming data based on our assumptions and the data we have with us. In prediction, we predict the data based on the previous data we have with us and based on the assumptions. For example, we can forecast the likelihood of a particular outcome, such as whether or not a customer will churn in 30 days.
Video 4: Classification Tasks
So, in this video, we'll be talking about the classification class. There are four types of classification tasks that we may encounter in real life. These are binary classification, multiclass classification, multi-label classification and imbalanced classification. Binary classification is the task when we have two class labels.
3
Example, cancer detection, cancer patient or not. Conversion prediction, buy or not. Email spam detection, spam or harm. Binary classification tasks typically involve a normal state and an abnormal state which are the two classes. For example, not spam is the normal state and spam is the abnormal state. Similarly, cancer not detected is the normal state and cancer detected is the abnormal state. The class for the normal state can be assigned a class label one and the class with abnormal state can be assigned a class label zero.
The Bernoulli distribution is a discrete probability distribution that covers a case where an event will have a binary outcome either as a zero or one. We can model a binary classification task with a model that predicts a Bernoulli probability distribution for each example. This means that the model predicts the probability of an example belonging to the abnormal state. Popular binary classification algorithms include naive bayes, support-vector machine, logistic regression, K-nearest neighbours and decision trees.
The logistic regression and support-vector machines do not support more than two classes. Now, let us look at the multiclass classification. Multiclass classification involves classification tasks that have more than two class labels. Examples include optical character recognition, face recognition, plant species classification. In multiclass classification, there is no notion of normal and abnormal outcomes.
Here is an example, which is classified as belonging to one among a range of non classes. In face recognition, the model may be predicting a photo as belonging to one among thousands of faces in the face recognition system. Predicting a sequence of words in text translation models is a type of multiclass classification. Here, the size of the vocabulary defines the number of classes that may be predicted and could be many thousands of words in size.
A multiclass classification task can be modelled with a model that predicts a multinolli probability distribution for each example. This means that the multiclass classification model predicts the probability of an example belonging to each class label. The popular multiclass classification algorithms include naive bayes, K-nearest neighbours, decision trees, random forest and gradient boosting. The third type is the multi-label classification.
In multi-label classification, there will be two or more class labels where one or more class labels may be predicted for each case. For example, in image classification, a given image may have multiple objects in a scene and the multi-label model may predict the presence of multiple known objects in the image such as bike, car, person and so on. In binary classification and multiclass classification, only a single class label is predicted in each case.
We can model a multi-label classification task with a model that predicts multiple outputs with each output prediction coming from a Bernoulli probability distribution. Multi-label versions of the standard classification algorithms are needed for multi-label classification, such as multi-label decision trees, multi-label gradient boosting, multi-label random forest. In imbalanced classification, the number of examples in each class is imbalanced or unequally distributed.
Typically, they are binary classification tasks. The majority of the examples in the training data set will belong to the normal class and only a minority of the examples will be in the abnormal class. For example, brain tumor detection or financial fraud detection or outlier
4
detection.